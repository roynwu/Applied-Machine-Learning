{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "linear+poly_regression.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0k5niFD-iMy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "import numpy as np\n",
        "from numpy import linalg as LA\n",
        "from numpy.linalg import *\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "Collapsed": "false",
        "id": "k3oFyr_wzHxV",
        "colab_type": "text"
      },
      "source": [
        "# Linear Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "Collapsed": "false",
        "id": "9Y30de0dzHxW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "    Linear Regression via Gradient Descent\n",
        "'''\n",
        "\n",
        "class LinearRegression:\n",
        "\n",
        "    def __init__(self, init_theta=None, alpha=0.01, n_iter=100):\n",
        "        '''\n",
        "        Constructor\n",
        "        '''\n",
        "        self.alpha = alpha\n",
        "        self.n_iter = n_iter\n",
        "        self.theta = init_theta\n",
        "        self.JHist = None\n",
        "    \n",
        "\n",
        "    def gradientDescent(self, X, y, theta):\n",
        "        '''\n",
        "        Fits the model via gradient descent\n",
        "        Arguments:\n",
        "            X is a n-by-d numpy matrix\n",
        "            y is an n-dimensional numpy vector\n",
        "            theta is a d-dimensional numpy vector\n",
        "        Returns:\n",
        "            the final theta found by gradient descent\n",
        "        '''\n",
        "        n,d = X.shape\n",
        "        self.JHist = []\n",
        "        for i in range(self.n_iter):\n",
        "            self.JHist.append( (self.computeCost(X, y, theta), theta) )\n",
        "            # print(\"Iteration: \", i+1, \" Cost: \", self.JHist[i][0], \" Theta.T: \", theta.T)\n",
        "            yhat = X*theta\n",
        "            theta = theta -  (X.T * (yhat - y)) * (self.alpha / n)\n",
        "        return theta\n",
        "    \n",
        "\n",
        "    def computeCost(self, X, y, theta):\n",
        "        '''\n",
        "        Computes the objective function\n",
        "        Arguments:\n",
        "          X is a n-by-d numpy matrix\n",
        "          y is an n-dimensional numpy vector\n",
        "          theta is a d-dimensional numpy vector\n",
        "        Returns:\n",
        "          a scalar value of the cost  \n",
        "              ** Not returning a matrix with just one value! **\n",
        "        '''\n",
        "        n,d = X.shape\n",
        "        yhat = X*theta\n",
        "        J =  (yhat-y).T * (yhat-y) / n\n",
        "        J_scalar = J.tolist()[0][0]  # convert matrix to scalar\n",
        "        return J_scalar\n",
        "    \n",
        "\n",
        "    def fit(self, X, y):\n",
        "        '''\n",
        "        Trains the model\n",
        "        Arguments:\n",
        "            X is a n-by-d Pandas Dataframe\n",
        "            y is an n-dimensional Pandas Series\n",
        "        '''\n",
        "        n = len(y)\n",
        "        X = X.to_numpy()\n",
        "        X = np.c_[np.ones((n,1)), X]     # Add a row of ones for the bias term\n",
        "        print(X)\n",
        "        \n",
        "        y = y.to_numpy()\n",
        "        n,d = X.shape\n",
        "        y = y.reshape(n,1)\n",
        "\n",
        "        if self.theta is None:\n",
        "            self.theta = np.matrix(np.zeros((d,1)))\n",
        "\n",
        "        self.theta = self.gradientDescent(X,y,self.theta)   \n",
        "\n",
        "\n",
        "    def predict(self, X):\n",
        "        '''\n",
        "        Used the model to predict values for each instance in X\n",
        "        Arguments:\n",
        "            X is a n-by-d Pandas DataFrame\n",
        "        Returns:\n",
        "            an n-dimensional numpy vector of the predictions\n",
        "        '''\n",
        "        X = X.to_numpy()\n",
        "        X = np.c_[np.ones((n,1)), X]     # Add a row of ones for the bias term\n",
        "        return pd.DataFrame(X*self.theta)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "Collapsed": "false",
        "id": "nzRRQQokzHxZ",
        "colab_type": "text"
      },
      "source": [
        "### Test code for linear regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "Collapsed": "false",
        "scrolled": true,
        "id": "wGj6YqlLzHxa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_linreg(n_iter = 2000):\n",
        "  # load the data\n",
        "  filepath = \"http://www.seas.upenn.edu/~cis519/spring2020/data/hw2-multivariateData.csv\"\n",
        "  df = pd.read_csv(filepath, header=None)\n",
        "\n",
        "  X = df[df.columns[:-1]]\n",
        "  y = df[df.columns[-1]]\n",
        "\n",
        "  n,d = X.shape\n",
        "\n",
        "  # # Standardize features\n",
        "  from sklearn.preprocessing import StandardScaler\n",
        "  standardizer = StandardScaler()\n",
        "  X = pd.DataFrame(standardizer.fit_transform(X))  # compute mean and stdev on training set for standardization\n",
        "\n",
        "  # # initialize the model\n",
        "  init_theta = np.matrix(np.random.randn((d+1))).T\n",
        "  alpha = 0.01\n",
        "\n",
        "  # # Train the model\n",
        "  lr_model = LinearRegression(init_theta = init_theta, alpha = alpha, n_iter = n_iter)\n",
        "  lr_model.fit(X,y)\n",
        "\n",
        "  # # Compute the closed form solution\n",
        "  X = np.asmatrix(X.to_numpy())\n",
        "  X = np.c_[np.ones((n,1)), X]     # Add a row of ones for the bias term\n",
        "  y = np.asmatrix(y.to_numpy())\n",
        "  n,d = X.shape\n",
        "  y = y.reshape(n,1)\n",
        "  thetaClosedForm = inv(X.T*X)*X.T*y\n",
        "  print(\"thetaClosedForm: \", thetaClosedForm.T)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdkpNryXNOU9",
        "colab_type": "text"
      },
      "source": [
        "# Run the Linear Regression Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eT1J1MGUNM_I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_linreg(2000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "Collapsed": "false",
        "id": "dczVDMsRzHxe",
        "colab_type": "text"
      },
      "source": [
        "# Polynomial Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VtyI1YlYkcFb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "#-----------------------------------------------------------------\n",
        "#  Class PolynomialRegression\n",
        "#-----------------------------------------------------------------\n",
        "\n",
        "class PolynomialRegression:\n",
        "\n",
        "    def __init__(self, degree = 1, regLambda = 1E-8, tuneLambda = False, regLambdaValues = None):\n",
        "        '''\n",
        "        Constructor\n",
        "        '''\n",
        "        #TODO\n",
        "        self.degree = degree\n",
        "        self.regLambda = regLambda\n",
        "        self.tuneLambda = tuneLambda\n",
        "        self.regLambdaValues = regLambdaValues\n",
        "        self.theta = np.matrix(np.zeros((degree+1,1)))\n",
        "        self.alpha = 0.25\n",
        "        self.epsilon = 0.0001\n",
        "        self.max_iter = 100000\n",
        "\n",
        "    def polyfeatures(self, X, degree):\n",
        "        '''\n",
        "        Expands the given X into an n * d array of polynomial features of\n",
        "            degree d.\n",
        "\n",
        "        Returns:\n",
        "            A n-by-d data frame, with each column comprising of\n",
        "            X, X * X, X ** 3, ... up to the dth power of X.\n",
        "            Note that the returned matrix will not include the zero-th power.\n",
        "\n",
        "        Arguments:\n",
        "            X is an n-by-1 data frame\n",
        "            degree is a positive integer\n",
        "        '''\n",
        "        #TODO\n",
        "        X = np.array(X)\n",
        "        n = X.shape[0]\n",
        "\n",
        "        poly_features = np.zeros((n, degree))\n",
        "\n",
        "        for i in range(n):\n",
        "          x = X[i]\n",
        "\n",
        "          for j in range(degree):\n",
        "            # poly_features[i, j] = x ** (j + 1)\n",
        "            poly_features[i, j] = np.power(x, j + 1)\n",
        "\n",
        "        poly_features = pd.DataFrame(poly_features)\n",
        "\n",
        "        return poly_features\n",
        "        \n",
        "    def computeCost(self, X, y, theta, regLambda):\n",
        "\n",
        "        n,d = X.shape\n",
        "        # yhat = X*theta\n",
        "        yhat = X.dot(theta)\n",
        "        # J =  (yhat-y).T * (yhat-y) / n + regLambda * (theta.T * theta)\n",
        "        J =  ((((yhat-y).T).dot(yhat-y)) / n) + (regLambda * (theta.T).dot(theta))\n",
        "        J_scalar = J.tolist()[0][0]  # convert matrix to scalar\n",
        "\n",
        "        return J_scalar\n",
        "\n",
        "    def gradientDescent(self, X, y, theta, regLambda):\n",
        "\n",
        "        theta_old = theta\n",
        "        n,d = X.shape\n",
        "        self.JHist = []\n",
        "\n",
        "        # i = 0\n",
        "        # while True:\n",
        "        for i in range(self.max_iter):\n",
        "            # self.JHist.append( (self.computeCost(X, y, theta_old, regLambda), theta_old) )\n",
        "            # print(\"Iteration: \", i+1, \" Cost: \", self.JHist[i][0], \" Theta.T: \", theta_old.T)\n",
        "            # yhat = X*theta_old\n",
        "            yhat = X.dot(theta_old)\n",
        "            # theta_new = theta_old - (X.T * (yhat - y)) * (self.alpha / n) - self.alpha * regLambda * theta_old\n",
        "            # theta_new = theta_old - (((X.T).dot(yhat-y)) * (self.alpha / n)) - ((self.alpha * regLambda) * theta_old)\n",
        "\n",
        "            regulate = regLambda * self.alpha * theta_old\n",
        "            regulate[0] = 0 \n",
        "            theta_new = theta_old - ((X.T).dot(yhat - y)) * (self.alpha / (2*n)) - regulate\n",
        "            # theta_new = theta_old - ((X.T).dot(yhat - y)) * (self.alpha / n) - regulate\n",
        "\n",
        "            if LA.norm(theta_new - theta_old) < self.epsilon:\n",
        "              break\n",
        "\n",
        "            else:\n",
        "              theta_old = theta_new\n",
        "              # i += 1\n",
        "\n",
        "        return theta_old\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        '''\n",
        "            Trains the model\n",
        "            Arguments:\n",
        "                X is a n-by-1 data frame\n",
        "                y is an n-by-1 data frame\n",
        "            Returns:\n",
        "                No return value\n",
        "            Note:\n",
        "                You need to apply polynomial expansion and scaling first\n",
        "        '''\n",
        "        #TODO\n",
        "        n = len(y)\n",
        "        X = np.array(X)\n",
        "        X = np.array(self.polyfeatures(X, self.degree))\n",
        "        mean = np.mean(X, axis=0)\n",
        "        std = np.std(X, axis=0)\n",
        "        X = (X - mean) / std\n",
        "        X = np.c_[np.ones((n, 1)), X]\n",
        "\n",
        "        y = np.array(y)\n",
        "        n, d = X.shape\n",
        "        y = y.reshape(n,1)\n",
        "\n",
        "        # if self.theta is None:\n",
        "        #     self.theta = np.matrix(np.zeros((d,1)))\n",
        "\n",
        "        # self.theta = self.gradientDescent(X,y,self.theta,self.regLambda)\n",
        "\n",
        "        if self.tuneLambda == True:\n",
        "          dictionary = {}\n",
        "          for lambda_value in self.regLambdaValues:\n",
        "            thetahat = self.gradientDescent(X,y,self.theta,lambda_value)\n",
        "\n",
        "            yhat = X*thetahat\n",
        "            RMSE = np.sqrt(mean_squared_error(y, yhat))\n",
        "            dictionary[lambda_value] = RMSE\n",
        "          \n",
        "          lambda_optimal = min(dictionary, key=dictionary.get)\n",
        "          print('Optimal Lambda: ' + str(lambda_optimal))\n",
        "          print('Lambda values; RMSE: ' + str(dictionary))\n",
        "\n",
        "          self.theta = np.matrix(np.zeros((d,1)))\n",
        "          self.theta = self.gradientDescent(X,y,self.theta,lambda_optimal)\n",
        "          \n",
        "          #   # Loop through trials\n",
        "          #   # for i in range(5):\n",
        "          #     dataset = np.concatenate((X,y.reshape(-1,1)),axis=1)\n",
        "          #     # Store each cv score in list\n",
        "          #     scores = []\n",
        "          #     # Split dataset into folds\n",
        "          #     # np.random.shuffle(dataset)\n",
        "          #     data_split = np.array_split(dataset, 3)\n",
        "\n",
        "          #     # Loop through folds\n",
        "          #     for k in range(len(data_split)):\n",
        "      \n",
        "          #       # Test set\n",
        "          #       test_set = data_split[k]\n",
        "\n",
        "          #       # Train set\n",
        "          #       train_lst = []\n",
        "          #       for i in range(len(data_split)):\n",
        "          #         if i != k:\n",
        "          #           train_lst.append(data_split[i])\n",
        "          #       train_set = np.concatenate(train_lst,axis=0)\n",
        "\n",
        "          #       # Split into X,y\n",
        "          #       X_train = train_set[:,:-1]\n",
        "          #       y_train = train_set[:,-1]\n",
        "          #       X_test = test_set[:,:-1]\n",
        "          #       y_test = test_set[:,-1]\n",
        "\n",
        "          #       # Score\n",
        "          #       thetahat = self.gradientDescent(X_train,y_train,self.theta,self.regLambda)\n",
        "          #       yhat = X_test.dot(thetahat)\n",
        "          #       RMSE = np.sqrt(mean_squared_error(y_test, yhat))\n",
        "          #       scores.append(RMSE)\n",
        "\n",
        "          #     dictionary[lambda_value] = np.mean(scores)\n",
        "\n",
        "          # lambda_optimal = min(dictionary, key=dictionary.get)\n",
        "          # self.theta = self.gradientDescent(X,y,self.theta,lambda_optimal)\n",
        "\n",
        "        else:\n",
        "          self.theta = self.gradientDescent(X,y,self.theta,self.regLambda)\n",
        "          # self.theta = np.matmul(np.matmul(linalg.pinv(np.matmul(np.transpose(X),X)), np.transpose(X)), y)\n",
        "          # self.theta = np.dot(np.dot(np.linalg.pinv(X.T * X), X.T), y)\n",
        "          # self.theta = np.linalg.pinv(X.T.dot(X)).dot(X.T).dot(y) \n",
        "          print(self.theta)\n",
        "  \n",
        "    def predict(self, X):\n",
        "        '''\n",
        "        Use the trained model to predict values for each instance in X\n",
        "        Arguments:\n",
        "            X is a n-by-1 data frame\n",
        "        Returns:\n",
        "            an n-by-1 data frame of the predictions\n",
        "        '''\n",
        "        # TODO\n",
        "        X = np.array(X)\n",
        "        X = self.polyfeatures(X, self.degree)\n",
        "        mean = np.mean(X, axis=0)\n",
        "        std = np.std(X, axis=0)\n",
        "        X = (X - mean) / std\n",
        "        n, d = X.shape\n",
        "        X = np.c_[np.ones((n, 1)), X]\n",
        "\n",
        "        # predictions = pd.DataFrame(X * self.theta)\n",
        "        predictions = pd.DataFrame(X.dot(self.theta))\n",
        "\n",
        "        return predictions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZlhLMhYQOpM",
        "colab_type": "text"
      },
      "source": [
        "# Test Polynomial Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqAzS9tC1AlJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def test_polyreg_univariate():\n",
        "    '''\n",
        "        Test polynomial regression\n",
        "    '''\n",
        "\n",
        "    # load the data\n",
        "    filepath = \"http://www.seas.upenn.edu/~cis519/spring2020/data/hw2-polydata.csv\"\n",
        "    df = pd.read_csv(filepath, header=None)\n",
        "\n",
        "    X = df[df.columns[:-1]]\n",
        "    y = df[df.columns[-1]]\n",
        "\n",
        "    # regression with degree = d\n",
        "    d = 8\n",
        "    regLambdaList = [0, 0.001, 0.003, 0.006, 0.01, 0.03, 0.006, 0.1, 0.3, 0.6, 1, 3]\n",
        "    model = PolynomialRegression(degree = d, regLambda = 0, tuneLambda = True, regLambdaValues = regLambdaList)\n",
        "    # model = PolynomialRegression(degree = d, regLambda = 1E-8, tuneLambda = False, regLambdaValues = regLambdaList)\n",
        "    model.fit(X, y)\n",
        "    \n",
        "    # output predictions\n",
        "    xpoints = pd.DataFrame(np.linspace(np.max(X), np.min(X), 100))\n",
        "    ypoints = model.predict(xpoints)\n",
        "\n",
        "    # plot curve\n",
        "    plt.figure()\n",
        "    plt.plot(X, y, 'rx')\n",
        "    plt.title('PolyRegression with d = '+str(d))\n",
        "    plt.plot(xpoints, ypoints, 'b-')\n",
        "    plt.xlabel('X')\n",
        "    plt.ylabel('Y')\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48FJF0yAt1qw",
        "colab_type": "code",
        "outputId": "15aaa1f5-0881-4ed2-81c5-bad897e15742",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        }
      },
      "source": [
        "test_polyreg_univariate()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Optimal Lambda: 0\n",
            "Lambda values; RMSE: {0: 11.321393992455604, 0.001: 11.893192756799126, 0.003: 12.25795902834811, 0.006: 12.428922300277415, 0.01: 12.535703447535157, 0.03: 12.748039848109089, 0.1: 12.968310988708781, 0.3: 13.214692456463103, 0.6: 13.400481996262457, 1: 13.560132987700714, 3: 14.098778854318041}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAEWCAYAAACNJFuYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GhYUcXftsD6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}